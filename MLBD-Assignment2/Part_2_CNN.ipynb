{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Convolutional Neural Networks\n",
    "\n",
    "Daniel-Alexandru Bejan (474404)\n",
    "Patrick Schaper (534366)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment you are going to use the dataset IML-2022-Anglo-Saxion-Runes\n",
    "from Introduction to Machine Learning (see BlackBoard).\n",
    "\n",
    "There are plenty of examples of how to build convolutional neural networks. We advice\n",
    "you, however, to reuse the code from your first assignment. \n",
    "\n",
    "This time you also need to\n",
    "use convolutional layers and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, json, shutil, pprint\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import IPython.display as display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.callbacks import History\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(tf.__version__)\n",
    "\n",
    "def display(np_image):\n",
    "    \"\"\"\n",
    "    This is a display function that we have added to show numpy images at full size\n",
    "    If you pass in an image with 3 channels, it will be displayed in RGB\n",
    "    If you passn in an image with 1 channel, it will be displayed in grayscale\n",
    "    \"\"\"\n",
    "    dpi = matplotlib.rcParams['figure.dpi']\n",
    "    if len(np_image.shape) == 3:\n",
    "        height, width, depth = np_image.shape\n",
    "    else:\n",
    "        height, width = np_image.shape\n",
    "\n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "    # Create a figure of the right size with one axis that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display the image in either RGB or grayscale (depending on the amount of dimensions)\n",
    "    if (len(np_image.shape) >= 3):\n",
    "        ax.imshow(np_image)\n",
    "    else:\n",
    "        ax.imshow(np_image, cmap='gray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 3030\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 128x128 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 4)\n"
     ]
    }
   ],
   "source": [
    "#change the color to gray picture\n",
    "from skimage.color import rgb2gray\n",
    "path = './MLBD-dataset-IML-2022-Anglo-Saxion-Runes'\n",
    "images_original = io.imread_collection(os.path.join(f'{path}/*.png'))\n",
    "print(f'Number of images: {len(images_original)}')\n",
    "display(images_original[0])\n",
    "print(images_original[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new dataframe with the labels and the image names separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_grayscale = []\n",
    "images_as_gray = []\n",
    "for file_name in listdir(path):\n",
    "    full_path = join(path, file_name)\n",
    "    if isfile(full_path):\n",
    "        if re.match(r\".*\\.png$\", full_path):\n",
    "            image_current = io.imread(full_path, as_gray=True)\n",
    "            image_label = file_name.split('_')[0].lower()\n",
    "            image_number = file_name.split('_')[2].lower() + \"_\" + file_name.split('_')[3].lower()\n",
    "            \n",
    "            image_definition = [image_current, image_number, image_label]\n",
    "            \n",
    "            dataset_grayscale.append(image_definition)\n",
    "\n",
    "labeled_images = pd.DataFrame(dataset_grayscale, columns=[\"image\", \"rune_id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>rune_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_2025030270.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_2549270937.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_3669898631.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_4105113150.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_4272441664.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_4538430006.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_7060350912.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_7724475204.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_8403188040.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>160547_9446549830.png</td>\n",
       "      <td>ash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image                rune_id  \\\n",
       "0  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_2025030270.png   \n",
       "1  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_2549270937.png   \n",
       "2  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_3669898631.png   \n",
       "3  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_4105113150.png   \n",
       "4  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_4272441664.png   \n",
       "5  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_4538430006.png   \n",
       "6  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_7060350912.png   \n",
       "7  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_7724475204.png   \n",
       "8  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_8403188040.png   \n",
       "9  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  160547_9446549830.png   \n",
       "\n",
       "  label  \n",
       "0   ash  \n",
       "1   ash  \n",
       "2   ash  \n",
       "3   ash  \n",
       "4   ash  \n",
       "5   ash  \n",
       "6   ash  \n",
       "7   ash  \n",
       "8   ash  \n",
       "9   ash  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_images.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now, the images are stored as numpy arrays grayscale with 128x128 pixels and contains the rune id and the label as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in dataset 3030\n",
      "Shape (128, 128)\n",
      "Numpy Array [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "Rune id 161206_2085017735.png\n",
      "Label ash\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 128x128 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Number of images in dataset', len(dataset_grayscale))\n",
    "print('Shape', dataset_grayscale[200][0].shape)\n",
    "print('Numpy Array', dataset_grayscale[200][0])\n",
    "print('Rune id', dataset_grayscale[200][1])\n",
    "print('Label', dataset_grayscale[200][2])\n",
    "display(dataset_grayscale[200][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(labeled_images['image'].shape)\n",
    "print(type(labeled_images['image']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is stated into the the assignment we are going to experiment with different network structures. \n",
    "\n",
    "Therefore, we would have to:\n",
    "\n",
    "- Experiment with at least 6 different network structures with minimum of 2 convolutional layers per network.\n",
    "- Difference with and without dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "IMG_SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_original_np_array = np.array(images_original)\n",
    "# labels_fit_transform = LabelEncoder().fit_transform(labeled_images['label'])\n",
    "# data_set_categorial = tf.keras.utils.to_categorical(labels_fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_grayscale[:][0], dataset_grayscale[:][2], test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['160547_3669898631.png', 'ash'], dtype='<U21'),\n",
       " array([1, 1], dtype=int64))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_runes(runes, labels):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(runes[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the slides are saying, the convolutional layers are used to extract features from the images.\n",
    "We are going to define a function that will create this for us.\n",
    "\n",
    "Keras layers engine contains an implementation of the convolutional layer\n",
    "- Filters = number of filters you want to apply (= number of outputs)\n",
    "- Kernel size = size of the weights kernel\n",
    "- Padding = \"valid\" (don't pad) or \"same\" (output size equal to input size)\n",
    "- Strides = stride step size for vertical and horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_conv_layers(model, kernel_size=(3,3), pool_size=(2,2), activation='relu', dropout=0.25):\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size, padding='same', input_shape=(128, 128, 3)))\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size, padding='same'))\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size))\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "    # model.add(keras.layers.Conv2D(128, kernel_size, padding='same'))\n",
    "    # model.add(keras.layers.Activation(activation))\n",
    "    # model.add(keras.layers.Conv2D(128, kernel_size))\n",
    "    # model.add(keras.layers.Activation(activation))\n",
    "    # model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    # model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "    # model.add(keras.layers.Flatten())\n",
    "    # model.add(keras.layers.Dense(512))\n",
    "    # model.add(keras.layers.Activation(activation))\n",
    "    # model.add(keras.layers.Dropout(0.5))\n",
    "    # model.add(keras.layers.Dense(25))\n",
    "    # model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential()\n",
    "    model = define_conv_layers(model)\n",
    "    # Define the fully connected layer\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Define the output layer\n",
    "    model.add(tf.keras.layers.Dense(11, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # print model layers\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_model(input_data, output_data, layers=[], optimizer=\"adam\", endActi=\"softmax\", batchSize=50, epochs=10, verbose=0, withHistory=0):\n",
    "    model = create_model()\n",
    "\n",
    "#     inputNeurons = input_data.shape[1]\n",
    "    outputNeurons = 11\n",
    "\n",
    "    # output_data = output_data.values.reshape(-1,1)\n",
    "    # print(output_data[:][2])\n",
    "    # encoder = LabelEncoder().fit_transform(output_data[:][2])\n",
    "    # output_data = tf.keras.utils.to_categorical(output_data, outputNeurons)\n",
    "    y_test_categorical = tf.keras.utils.to_categorical(y_test, outputNeurons)\n",
    "    \n",
    "    history = History()\n",
    "    history = model.fit(input_data, \n",
    "                        y=output_data, \n",
    "                        batch_size=batchSize,\n",
    "                        epochs=epochs, \n",
    "                        # verbose=verbose, \n",
    "                        shuffle=True, \n",
    "                        # validation_data=(X_test, y_test_categorical), \n",
    "                        #                  callbacks=[history]\n",
    "                                                    )\n",
    "    if withHistory == 1:\n",
    "            return model, history\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history):\n",
    "    plt.figure(figsize=(20,8))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(input_data, learningRate = 0.03, optimizer=\"adam\", loss_function='categorical_crossentropy', activation_function=\"softmax\"):\n",
    "#     # Define the input layer\n",
    "#     input_layer = tf.keras.layers.Input(shape=IMG_SIZE) # 4 channels of RGB\n",
    "\n",
    "#     # Define the convolutional layers\n",
    "#     conv_layer_1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "#     pooling_layer_1 = tf.keras.layers.MaxPooling2D((2, 2))(conv_layer_1)\n",
    "#     conv_layer_2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(pooling_layer_1)\n",
    "#     pooling_layer_2 = tf.keras.layers.MaxPooling2D((2, 2))(conv_layer_2)\n",
    "#     conv_layer_3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(pooling_layer_2)\n",
    "#     pooling_layer_3 = tf.keras.layers.MaxPooling2D((2, 2))(conv_layer_3)\n",
    "\n",
    "#     # Flatten the output from the convolutional layers\n",
    "#     flattened_layer = tf.keras.layers.Flatten()(pooling_layer_3)\n",
    "\n",
    "#     # Define the fully connected layer\n",
    "#     fc_layer = tf.keras.layers.Dense(512, activation='relu')(flattened_layer)\n",
    "\n",
    "#     # Define the output layer\n",
    "#     output_layer = tf.keras.layers.Dense(11, activation='softmax')(fc_layer)\n",
    "\n",
    "#     # Create the model\n",
    "#     model = tf.keras.Model(input_layer, output_layer)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'], sample_weight_mode=\"temporary\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='conv2d_54_input'), name='conv2d_54_input', description=\"created by layer 'conv2d_54_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_13\" (type Sequential).\n    \n    Input 0 of layer \"conv2d_54\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[39m=\u001b[39m create_fit_model(input_data\u001b[39m=\u001b[39;49mX_train, output_data\u001b[39m=\u001b[39;49my_train, layers\u001b[39m=\u001b[39;49m[], optimizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m, endActi\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msoftmax\u001b[39;49m\u001b[39m\"\u001b[39;49m, batchSize\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, withHistory\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m plot_acc_loss(history)\n",
      "Cell \u001b[1;32mIn[144], line 14\u001b[0m, in \u001b[0;36mcreate_fit_model\u001b[1;34m(input_data, output_data, layers, optimizer, endActi, batchSize, epochs, verbose, withHistory)\u001b[0m\n\u001b[0;32m     11\u001b[0m y_test_categorical \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_test, outputNeurons)\n\u001b[0;32m     13\u001b[0m history \u001b[39m=\u001b[39m History()\n\u001b[1;32m---> 14\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(input_data, \n\u001b[0;32m     15\u001b[0m                     y\u001b[39m=\u001b[39;49moutput_data, \n\u001b[0;32m     16\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatchSize,\n\u001b[0;32m     17\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs, \n\u001b[0;32m     18\u001b[0m                     \u001b[39m# verbose=verbose, \u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m                     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[0;32m     20\u001b[0m                     \u001b[39m# validation_data=(X_test, y_test_categorical), \u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m                     \u001b[39m#                  callbacks=[history]\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m                                                 )\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m withHistory \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     24\u001b[0m         \u001b[39mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Software\\Anaconda\\envs\\project-bdt\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_13\" (type Sequential).\n    \n    Input 0 of layer \"conv2d_54\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "model, history = create_fit_model(input_data=X_train, output_data=y_train, layers=[], optimizer=\"adam\", endActi=\"softmax\", batchSize=16, epochs=200, verbose=0, withHistory=1)\n",
    "plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "76/76 [==============================] - 25s 327ms/step - loss: 0.2184 - accuracy: 0.9344\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 32s 427ms/step - loss: 0.1194 - accuracy: 0.9620\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 32s 422ms/step - loss: 0.0753 - accuracy: 0.9736\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 35s 458ms/step - loss: 0.0526 - accuracy: 0.9823\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 37s 482ms/step - loss: 0.0662 - accuracy: 0.9814\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 40s 531ms/step - loss: 0.0383 - accuracy: 0.9897\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 47s 620ms/step - loss: 0.0424 - accuracy: 0.9843\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 36s 464ms/step - loss: 0.0460 - accuracy: 0.9864\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 51s 677ms/step - loss: 0.1051 - accuracy: 0.9707\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 31s 405ms/step - loss: 0.1376 - accuracy: 0.9682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126b1076130>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-bdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cda239b1dd777d01df567143bbbda5f1e26f67cd0d962f9425eb41a04df5b565"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
